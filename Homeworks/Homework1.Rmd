---
title: "Predictive Analysis: First Half Semester Homework"
author: "Jered Ataky, Matthew Baker, Christopher Bloome, David Blumenstiel, Dhairav Chhatbar"
date: "6/6/2021"  
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Libraries

```{r warning=FALSE, message=FALSE}

library(fpp2)

```


## Problem 1: HA 2.1

Use the help function to explore what the series gold, woolyrnq and gas represent.

**Description of each of the series:**

gold: Daily morning gold prices in US dollars. 1 January 1985 – 31 March 1989.
woolyrnq: Quarterly production of woollen yarn in Australia: tonnes. Mar 1965 – Sep 1994.
gas: Australian monthly gas production: 1956–1995.


a. Use autoplot() to plot each of these in separate plots.

**Plotting gold**


```{r}

autoplot(gold)

```


**Plotting woolyrnq**

```{r}

autoplot(woolyrnq)

```


**Plotting gas**

```{r}

autoplot(gas)

```



b. What is the frequency of each series? Hint: apply the frequency() function.

```{r}

frequency(gold)

```

gold data are annual.


```{r}

frequency(woolyrnq)

```
woolyrnq data are quarterly


```{r}

frequency(gas)

```

gas data are monthly. 


c. Use which.max() to spot the outlier in the gold series. Which observation was it?


```{r}

g <- which.max(gold)
g

```
gold get the max value at 770

Calculating gold value at t = 770:


```{r}

gold[g]
```

gold maximum value is 593.7


## Problem 2: HA 2.3

Download some monthly Australian retail data from the book website. These represent retail sales in various categories for different Australian states, and are stored in a MS-Excel file.

a. You can read the data into R with the following script:

```{r}

retaildata <- readxl::read_excel("retail.xlsx", skip=1)

```

Check the head of the retail data:

```{r}

head(retaildata)

```

The second argument (skip=1) is required because the Excel sheet has two header rows.


b. Select one of the time series as follows (but replace the column name with your own chosen column):

WE are going to use the column "A3349398A"

```{r}

myts <- ts(retaildata[,"A3349398A"],
  frequency=12, start=c(1982,4))


```



c. Explore your chosen retail time series using the following functions:

autoplot(), ggseasonplot(), ggsubseriesplot(), gglagplot(), ggAcf()

Can you spot any seasonality, cyclicity and trend? What do you learn about the series?


**Exploring A3349398A:"**

Plotting myts autoplot

```{r}
autoplot(myts)

```


Plotting myts ggseasonplot

```{r}
ggseasonplot(myts)

```



Plotting myts ggsubseriesplot

```{r}
ggsubseriesplot(myts)

```


Plotting myts gglagplot

```{r}
gglagplot(myts)

```

Plotting mts ggAcf

```{r}
ggAcf(myts)

```


We can see that there is a long-term increase and the time series is affected by 
a seasonal factor. Thus we observe both the **trend** and **seasonality**


## Problem 3: HA 6.2


The plastics data set consists of the monthly sales (in thousands) of product A for a plastics manufacturer for five years.

a. Plot the time series of sales of product A. Can you identify seasonal fluctuations and/or a trend-cycle?


View the data
```{r}

plastics

```

Plotting tie series using autoplot function

```{r}

autoplot(plastics)

```

The data has a long-term increase so there is a trend, and it is also seasonal
with a peak around August and September


b. Use a classical multiplicative decomposition to calculate the trend-cycle and seasonal indices.


Decomposing the series...

```{r}
plastics %>%decompose(type="multiplicative")%>%autoplot()

```


c. Do the results support the graphical interpretation from part a?

Yes it does. It shows the increase on the trend and a seasonal fluctuations.  


d. Compute and plot the seasonally adjusted data.

We are going to use the classical multiplicative decomposition to compute for the 
seasonally adjusted data

```{r}

library(seasonal)

plastics %>% decompose(type="multiplicative") -> fit

autoplot(plastics, series = "Data") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") +
  ylab("Monthly Sale")

```

e. Change one observation to be an outlier (e.g., add 500 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?


We going to use 50th observation (February of the 5th year) and change it
by adding 500

```{r}

new_plastics <- plastics
new_plastics[50] <- new_plastics[50] + 500
new_plastics[50]

```

Recompute seasonally adjusted data:

```{r}


new_plastics %>% decompose(type="multiplicative") -> fit

autoplot(new_plastics, series = "Data") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") +
  ylab("Monthly Sale")

```


The outlier affects the seasonally adjusted more than it does on the trend, 
that's it, there is big change on the seasonally adjusted but the trend is
affected only where the outlier is located.


f. Does it make any difference if the outlier is near the end rather than in the middle of the time series?

To answer to this question, we are going to recompute seasonally adjusted data
with an outlier closer to the middle and compare it to the previous one where 
the outlier was closer to the end.



```{r}

new_plastics <- plastics
new_plastics[35] <- new_plastics[35] + 500

new_plastics %>% decompose(type="multiplicative") -> fit

autoplot(new_plastics, series = "Data") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") +
  ylab("Monthly Sale")

```

It makes a diffrence where the outlier is located.
If the outlier is near the end, the seasonally adjusted plot does not change a lot
meaning that the prior values are not impacted significantly compared to when the 
outlier is in the middle where the seasonally adjusted is affected more.


## Problem 4: KJ 3.1

The UC Irvine Machine Learning Repository6 contains a data set related
to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe

### Libraries

```{r, message=FALSE, warning=FALSE}

library(mlbench)

```


### The data

```{r}
data(Glass)

# Rename the data
df <- Glass

str(df)

```

### a.

Using visualizations, explore the predictor variables to understand their
distributions as well as the relationships between predictors.

We are going to plot histogram to understand the distributions. The histogram
would display the distribution (uniform or skewed).
For the relationship between predictors, we are going to find the 
correlation between them and plot a correlation matrix.

First, let plot histograms of each predictor:


```{r}
library(tidyverse)

df %>%
  gather(key, value, -c(Type)) %>%
  ggplot(aes(value)) +
  geom_histogram(bins = 20, fill = 'blue') +
  facet_wrap(~key, scales ='free', ncol=3)

```


Now, we are going to use corrplot to plot the correlation between predictors.

```{r}
library(corrplot)

# Drop column Type to remain with only the 9 predictors

df <- subset(df, select = -c (Type))

head(df)

```

Let calculate the correlation first
```{r}

correlations <- round(cor(df), 2)
correlations
```

Now, let plot it:

```{r}

corrplot(correlations)

```




### b.

Do there appear to be any outliers in the data? Are any predictors skewed?

Let plot boxplot to visualize the outliers

```{r}

#boxplot(df)

df %>%
  gather(key, value) %>%
  ggplot(aes(x=key, y=value)) +
  geom_boxplot() +
  facet_wrap(~key, scales ='free', ncol=3)

```

As we can see on the plot above, they appear to be outliers in the data.

One thing observed from the boxplot above is that Mg looks multimodel,
and does not present any outliers. We are going to plot its histogram below
to visualize it.

```{r}
hist(df$Mg, main="Mg distribution")

```


Let compute the skewness to find out if there are predictors skewed.


```{r}

skewValues <- apply(df, 2, skewness)
skewValues

```

From the skewness values, we can say that except Na which is a bit close to normal, 
all the other predictors are skewed with Si being the least skewed.


### c.

Are there any relevant transformations of one or more predictors that
might improve the classification model?

BoxCox would be a good transformation, and we can also do principle components
but he data should be centered and scaled first.

That's said, we are going to do BoxCox transformation.
As BoxCox can only be done for non null positive values, we will first add a 
very small values to predictors containing zeros prior applying the 
transformation.

```{r}

df$Ba <- df$Ba + 1.e-6
df$Fe <- df$Fe + 1.e-6
df$K <- df$K + 1.e-6
df$Mg <- df$Mg + 1.e-6

df_bx <- preProcess(df, method="BoxCox")
df_bx

```

Let visualize the change after applying the transformation

```{r}

y <- predict(df_bx, df)

skewValues2 <- apply(y, 2, skewness)
skewValues2
```



## Problem 5: KJ 3.2

The soybean data can also be found at the UC Irvine Machine Learning
Repository. Data were collected to predict disease in 683 soybeans. The 35
predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left
spots, mold growth). The outcome labels consist of 19 distinct classes.



### Explore the data

Let first get an idea on the variables

```{r}
data("Soybean")

df <- Soybean

str(df)

```

### a.

Investigate the frequency distributions for the categorical predictors. Are
any of the distributions degenerate in the ways discussed earlier in this
chapter?


We will filter for near-zero variance predictors to find distributions 
that are degenerated.

We will output the names of predictors degenerated.

```{r}

df_var <- nearZeroVar(df)

df_nz <- colnames(df)[df_var]

df_nz
```


### b.

Roughly 18 % of the data are missing. Are there particular predictors that
are more likely to be missing? Is the pattern of missing data related to
the classes?

First, we are going to identify first the number of missing in each variable
and arrange them in descending order of number of missing values

```{r}

rev(sort(colSums(sapply(df, is.na))))

```


Then displaying all the levels we have in "Class" feature

```{r}

levels(df$Class)

```

Finally, see class which has more missing values then others

```{r}

df$nan_class = apply(df[,-1], 1, function(x){sum(is.na(x)) >0})

table(df[, c(1, 34    )])

```


From the able above, we can see that the pattern of missing data related to
the classes.


### c. 

Develop a strategy for handling missing data, either by eliminating
predictors or imputation.


We are going to use the caret class preProcess which has the ability
to transform, center, scale, or impute values,...
That said, the strategy for handling missing data would be using 
K-nearest neighbors which is a method applied by
preProcess function.


```{r message=FALSE, warning=FALSE}

df_cleaned <- preProcess(df, method = c("knnImpute"))

# Check the number of missing values

sum(is.na(df_cleaned))
```





## Problem 6: HA 7.1

Consider the pigs series — the number of pigs slaughtered in Victoria each month.

a. Use the ses() function in R to find the optimal values of  
α and ℓ0, and generate forecasts for the next four months.

b. Compute a 95% prediction interval for the first forecast using  
y±1.96s where s is the standard deviation of the residuals. 
Compare your interval with the interval produced by R.





## Problem 7: HA 7.3


Modify your function from the previous exercise to return the sum of squared errors rather than the forecast of the next observation. Then use the optim() function to find the optimal values of  
α and ℓ0. Do you get the same values as the ses() function?




## Problem 8







## Problem 9







## Problem 10








## Problem 11

